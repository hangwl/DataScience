{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Classification on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       " 'test': <SplitInfo num_examples=10000, num_shards=1>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall take 10% of the training data to be our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=6000>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_validation = 0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation = tf.cast(num_validation, tf.int64)\n",
    "num_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = mnist_info.splits['test'].num_examples\n",
    "num_test = tf.cast(num_test, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we would like to scale our data in some way so that our results are numerically stable.\\\n",
    "In this case, we scale our data so that our inputs are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image/255\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_train_and_validation = mnist_train.map(scale)\n",
    "scaled_data_test = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "shuffled_data_train_and_validation = scaled_data_train_and_validation.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_data_train_and_validation.take(num_validation)\n",
    "train_data = shuffled_data_train_and_validation.skip(num_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation)\n",
    "test_data = scaled_data_test.batch(num_test)\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # the first layer (the input layer)\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n",
    "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n",
    "    # or (28x28x1,) = (784,) vector\n",
    "    # this allows us to actually create a feed forward neural network\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 1s - loss: 0.0846 - accuracy: 0.9744 - val_loss: 0.0946 - val_accuracy: 0.9717 - 1s/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "540/540 - 1s - loss: 0.0752 - accuracy: 0.9766 - val_loss: 0.0825 - val_accuracy: 0.9758 - 954ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "540/540 - 1s - loss: 0.0672 - accuracy: 0.9796 - val_loss: 0.0774 - val_accuracy: 0.9775 - 934ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "540/540 - 1s - loss: 0.0588 - accuracy: 0.9826 - val_loss: 0.0647 - val_accuracy: 0.9815 - 949ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "540/540 - 1s - loss: 0.0533 - accuracy: 0.9829 - val_loss: 0.0615 - val_accuracy: 0.9817 - 934ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "540/540 - 1s - loss: 0.0470 - accuracy: 0.9855 - val_loss: 0.0692 - val_accuracy: 0.9792 - 942ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "540/540 - 1s - loss: 0.0431 - accuracy: 0.9870 - val_loss: 0.0595 - val_accuracy: 0.9835 - 937ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "540/540 - 1s - loss: 0.0387 - accuracy: 0.9885 - val_loss: 0.0458 - val_accuracy: 0.9857 - 943ms/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "540/540 - 1s - loss: 0.0365 - accuracy: 0.9889 - val_loss: 0.0493 - val_accuracy: 0.9845 - 937ms/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "540/540 - 1s - loss: 0.0327 - accuracy: 0.9902 - val_loss: 0.0474 - val_accuracy: 0.9862 - 929ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bdc81c7550>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ac5d840b21b8489350802d2508d91468aea9f35896439d4634edeb7fa6fa44f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
